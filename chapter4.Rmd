# Clustering and classification

In this exercise, we use the Boston data set, which describes the housing Values in suburbs of Boston. The data frame has in total 506 observations and 14 variables. For complete descriptions please see:  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

```{r warning=FALSE}

#Load libraries
library(MASS)
library(tidyr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(GGally)

# load the data
data("Boston")

# explore the dataset
str(Boston)
dim(Boston)

```
Next, I will plot distributions of the variables and check the summaries of the variables. Looks like the data is not normally distributed but quite heavily skewed.

```{r}

#show graphical overview of the data
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar(fill="blue", color="blue", alpha=0.9)

# summaries of the variables in the data
summary(Boston)

```

Too see the relationships between the variables let's plot correlation matrix. The two variables "rad" - index of accessibility to radial highways and "tax" - full-value property-tax rate per \$10,000 respectively are most strongly correlated (r=0.91). Interestingly, the "crim" - per capita crime rate, has most strong positive correlation with these variables. 

```{r}

#calculate correlation matrix
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```
Next, I standardize the dataset and print out summaries of the scaled data. Looks like the distributions are still skewed, but variable values are scaled around zero (mean). Using the class function() we see that the data set is a matrix. Later we will want the data to be a data frame. Let's use as.data.frame() function to convert the boston_scaled to the right format.

```{r}

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```
Next, I create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). I use the quantiles as the break points in the categorical variable. The old crime rate variable is dropped from the data set and the categorical variable added to the scaled data set.

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Next, I will divide the dataset to train and test sets, so that 80% of the data belongs to the train set. 

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable 
and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.
```{r}

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```
Then I will predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. Looks like all the predictions are on the diagonal and quite good.  

```{r}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
Next I will reload the Boston dataset and standardize the dataset.

```{r}

data('Boston')
Boston = scale(Boston)
Boston <- as.data.frame(Boston)

```
Then I calculate the distances between the observations. Looks like the distances are clearly shorter than they should be.

```{r}

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

```

Here, I run k-means with three centers and draw correlation plots, with data point colors corresponding with clusters. 

```{r}
# k-means clustering
km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster) #whole dataset
pairs(Boston[6:10], col = km$cluster) #pair up columns 6 to 10

```

To determine the optimal number of clusters. I set the maximum number of clusters to 10 and calculate the total of within cluster sum of squares (WCSS). The WCSS drops radically after 1 or 2. Therefore I would say that 2 clusters is the optimal number of clusters. 

```{r}
#K-means might produce different results every time, because it randomly assigns the initial cluster centers. The function set.seed() can be used to deal with that.
set.seed(123)

#Trying first with max 10 clusters
k_max <- "10"

#When you plot the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically. 
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```

